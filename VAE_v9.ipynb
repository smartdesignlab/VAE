{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_v8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf7B2lLV2twc",
        "colab_type": "text"
      },
      "source": [
        "# Variational Autoencoder "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkHf8zQ32zuq",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://drive.google.com/uc?id=1OCoK4me3T3EcaEm6H-MobpVG45LrNRrh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U4QeAgGjkOD",
        "colab_type": "text"
      },
      "source": [
        "## TensorFlow 및 기타 라이브러리 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpEFgoQFjToT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import itertools as it\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "%matplotlib inline\n",
        "np.random.seed(2018)\n",
        "tf.set_random_seed(2018)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5e-I8YC22Xe",
        "colab_type": "text"
      },
      "source": [
        "## MNIST 데이터 셋 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXWD-Xixjkdi",
        "colab_type": "code",
        "outputId": "739186a8-0a56-4130-9b63-d65c881c652b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Load the MNIST data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
        "X_train, X_test = mnist.train.images, mnist.test.images\n",
        "print (X_train.shape, X_test.shape)\n",
        "labels_train = mnist.train.labels\n",
        "n_samples = int(mnist.train.num_examples)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0826 05:36:39.571681 140148113229696 deprecation.py:323] From <ipython-input-2-44680a9a146b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0826 05:36:39.583314 140148113229696 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0826 05:36:39.584111 140148113229696 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0826 05:36:39.867517 140148113229696 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0826 05:36:39.915051 140148113229696 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "(55000, 784) (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ-qSCo93JYN",
        "colab_type": "text"
      },
      "source": [
        "## Variational Autoencoder Class 생성\n",
        "\n",
        "![대체 텍스트](https://drive.google.com/uc?id=1iNNT4PBS3dUeJ4ZYPmqWaU_7-ZUJRYyg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJbSGixdkHsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder(object):\n",
        "    # 초기변수 정의\n",
        "    def __init__(self, n_layers=[784, 500, 500, 2], activefunction=tf.nn.softplus, learning_rate=0.001, batch_size=100, graph = tf.Graph()):\n",
        "        # 입력과 출력 변수 갯수\n",
        "        self.n_layers = n_layers\n",
        "        # activefunction: 비선형 활성화 함수 Softplus 사용, Range: (0, ∞), 매끄럽게 만든 ReLU 함수라고 할 수 있음. 함수의 식은 log(exp(x) + 1)\n",
        "        # activefunction2: 비선형 활성화 함수 sigmoid 사용, Range:(0,1)\n",
        "        self.activefunction = activefunction\n",
        "        self.activefunction2 = tf.nn.sigmoid\n",
        "        # hyperparameters 정의\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        # 텐서플로우 계산 그래프 함수 정의\n",
        "        self.graph = graph\n",
        "        \n",
        "        # 그래프 생성\n",
        "        with self.graph.as_default():\n",
        "            # 손실에 대한 최적화 알고리즘 정의(AdamOptimizer사용)\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "            # 변수 초기화 함수 정의(모델 실행전 모든 변수 초기화) \n",
        "            network_weights = self._initialize_weights()\n",
        "            self.weights = network_weights\n",
        "            \n",
        "            # VAE 모델 생성\n",
        "            self.x = tf.placeholder(tf.float32, [None, self.n_layers[0]])\n",
        "            x = self.x        \n",
        "            \n",
        "            # Encoder network 생성(=inference Modell)\n",
        "            #  Gaussian MLP as encoder\n",
        "            self.hidden_encode = []\n",
        "            encoder_h1 = self.activefunction(\n",
        "                tf.add(tf.matmul(x, self.weights['encode'][0]['w']),\n",
        "                       self.weights['encode'][0]['b']))\n",
        "            encoder_h2 = self.activefunction(\n",
        "                tf.add(tf.matmul(encoder_h1, self.weights['encode'][1]['w']),\n",
        "                       self.weights['encode'][1]['b']))\n",
        "            \n",
        "            \n",
        "            # Encoder network의 마지막 hidden layer의 평균(mean)과 표준편차(log_sigma) 정의  \n",
        "            # sigma(표준편차)의 경우 항상 0보다 크거나 같아야하는데 마지막 hidden layer 연산을 한 encoder 층을 지나면 -값이 될 수 있기 때문에 이 값을 sigma로 보지 않고 log sigma라고 봄\n",
        "            self.mean = tf.add(\n",
        "                tf.matmul(encoder_h2, self.weights['mean']['w']), \n",
        "                self.weights['mean']['b'])\n",
        "            self.log_sigma = tf.add(\n",
        "                tf.matmul(encoder_h2, self.weights['log_sigma']['w']), \n",
        "                self.weights['log_sigma']['b'])\n",
        "            \n",
        "            # Reparameterization Trick사용을 위해 Gussian(normal)분포에서 랜덤변수(sample) ε추출 \n",
        "            eps = tf.random_normal(tf.stack([tf.shape(self.x)[0], self.n_layers[-1]]), 0, 1, dtype = tf.float32)            \n",
        "            \n",
        "            # z 정의\n",
        "            self.z = tf.add(self.mean, tf.multiply(tf.exp(self.log_sigma), eps))\n",
        "            z = self.z     \n",
        "            \n",
        "            # Decoder network 생성(=generator model)\n",
        "            # Bernoulli MLP as decoder\n",
        "            self.hidden_decoder = []  \n",
        "            \n",
        "            decoder_h1 = self.activefunction(\n",
        "                    tf.add(tf.matmul(z, self.weights['decode'][0]['w']),\n",
        "                        self.weights['decode'][0]['b']))\n",
        "            \n",
        "            decoder_h2 = self.activefunction(\n",
        "                    tf.add(tf.matmul(decoder_h1, self.weights['decode'][1]['w']),\n",
        "                        self.weights['decode'][1]['b']))\n",
        "            \n",
        "            # Decoder의 마지막 layer를 x_hat로 정의(x_hat = reconstruction x)\n",
        "            #  x_hat은 베르누이 분포를 따르도록 함. 데이터 분포의 평균을 (0,1)범위로 출력\n",
        "            self.x_hat = self.activefunction2(\n",
        "                    tf.add(tf.matmul(decoder_h2, self.weights['decode'][2]['w']),\n",
        "                        self.weights['decode'][2]['b']))  \n",
        "            \n",
        "            # Loss fuction 정의            \n",
        "            # (1) Marginal_likelihood(Reconstruct Error) : Cross-entropy \n",
        "            #   ln(0)=-Infinity을 피하기 위해 1e-10 추가\n",
        "            self.marginal_likelihood =  tf.reduce_sum(self.x * tf.log(1e-10 + self.x_hat) + (1-self.x) * tf.log(1e-10 + 1 - self.x_hat),1)\n",
        "            \n",
        "            # (2) KL divergence(Regularization,latent_loss)\n",
        "            self.KL_divergence = 0.5 * tf.reduce_sum(tf.square(self.mean)\n",
        "                                            + tf.exp(self.log_sigma)- 2*self.log_sigma -1, 1)\n",
        "            # (3) ELBO(VAE Loss function)\n",
        "            self.ELBO = tf.reduce_mean(self.marginal_likelihood - self.KL_divergence) # max ELBO\n",
        "            self.optimizer = optimizer.minimize(-self.ELBO) # min(-ELBO)\n",
        "            \n",
        "            # Create a saver(saver는 모든 변수를 각자의 이름으로 저장 및 복원)\n",
        "            self.saver = tf.train.Saver()\n",
        "            # 세션 실행전 변수 초기화 함수 정의\n",
        "            self.init = tf.global_variables_initializer()\n",
        "        \n",
        "        # 세션 정의\n",
        "        self.sess = tf.Session(graph=self.graph)\n",
        "        # 세션 실행(변수 초기화)\n",
        "        self.sess.run(self.init)\n",
        "        \n",
        "    # VAE 모델 초기화 함수 정의 \n",
        "    def _initialize_weights(self):\n",
        "        all_weights = dict()\n",
        "        initializer = tf.contrib.layers.xavier_initializer()\n",
        "        # Encoding network weights\n",
        "        Encoder_weights = []\n",
        "        for layer in range(len(self.n_layers)-2): # 4-2=2: 마지막 제외한 hidden layer 갯수\n",
        "            \n",
        "            w = tf.Variable(\n",
        "                initializer((self.n_layers[layer], self.n_layers[layer + 1]),\n",
        "                            dtype=tf.float32))\n",
        "            \n",
        "            b = tf.Variable(\n",
        "                tf.zeros([self.n_layers[layer + 1]], dtype=tf.float32))\n",
        "            Encoder_weights.append({'w': w, 'b': b})\n",
        "            \n",
        "        # Z(Latent variable)layer weights\n",
        "        w = tf.Variable(\n",
        "            initializer((self.n_layers[-2], self.n_layers[-1]),\n",
        "                        dtype=tf.float32))\n",
        "       \n",
        "        b = tf.Variable(\n",
        "            tf.zeros([self.n_layers[-1]], dtype=tf.float32))\n",
        "        all_weights['mean'] = {'w': w, 'b': b}\n",
        "       \n",
        "        w = tf.Variable(\n",
        "            initializer((self.n_layers[-2], self.n_layers[-1]),\n",
        "                        dtype=tf.float32))\n",
        "       \n",
        "        b = tf.Variable(\n",
        "            tf.zeros([self.n_layers[-1]], dtype=tf.float32))\n",
        "      \n",
        "        all_weights['log_sigma'] = {'w': w, 'b': b}\n",
        "      \n",
        "        # Decoder network weights\n",
        "        Decoder_weights = []\n",
        "        for layer in range(len(self.n_layers)-1, 0, -1): # range(start, limit, delta) => [3,2,1,0]\n",
        "            w = tf.Variable(\n",
        "                initializer((self.n_layers[layer], self.n_layers[layer - 1]),\n",
        "                            dtype=tf.float32))\n",
        "           \n",
        "            b = tf.Variable(\n",
        "                tf.zeros([self.n_layers[layer - 1]], dtype=tf.float32))\n",
        "          \n",
        "            Decoder_weights.append({'w': w, 'b': b})\n",
        "        all_weights['encode'] = Encoder_weights\n",
        "        all_weights['decode'] = Decoder_weights\n",
        "        return all_weights\n",
        "   \n",
        "    # 미니배치 모델을 훈련시킴(cost 반환) \n",
        "    def fit(self, X):\n",
        "        opt, cost = self.sess.run((self.optimizer, self.ELBO),feed_dict={self.x: X})\n",
        "        return -cost\n",
        "    \n",
        "    # Transform data by mapping it into the latent space.\n",
        "    def transform(self, X):\n",
        "        return self.sess.run(self.mean, feed_dict={self.x: X})\n",
        "    \n",
        "    # Generate data by sampling from latent space.\n",
        "    def generate(self, hidden = None):\n",
        "        if hidden is None:\n",
        "            hidden = np.random.randn(1, self.n_layers[-1])\n",
        "        # Note: This maps to mean of distribution, we could alternatively\n",
        "        #  sample from Gaussian distribution    \n",
        "        return self.sess.run(self.x_hat, feed_dict={self.z: hidden}) # x_hat: Decoder의 마지막 layer (x_hat = reconstruction x)\n",
        "      \n",
        "    # Use VAE to reconstruct given data.\n",
        "    def reconstruct(self, X):\n",
        "        return self.sess.run(self.x_hat, feed_dict={self.x: X}) # x_hat: Decoder의 마지막 layer (x_hat = reconstruction x)\n",
        "\n",
        "    def save(self, path):\n",
        "        '''To save trained model and its params.\n",
        "        '''\n",
        "        save_path = self.saver.save(self.sess, \n",
        "            os.path.join(path, 'model.ckpt'))\n",
        "        # save parameters of the model\n",
        "        params = {'n_layers': self.n_layers, 'learning_rate': self.learning_rate}\n",
        "        json.dump(params, \n",
        "            open(os.path.join(path, 'model_params.json'), 'w'))\n",
        "        return save_path\n",
        "    \n",
        "    # 실행 그래프 저장\n",
        "    def _restore(self, path):\n",
        "        with self.graph.as_default():\n",
        "            self.saver.restore(self.sess, path)\n",
        "\n",
        "    @classmethod\n",
        "    # To restore a saved model\n",
        "    def load(cls, path):\n",
        "        # load params of the model\n",
        "        params = json.load(open(os.path.join(path, 'model_params.json'), 'r'))\n",
        "        # init an instance of this class\n",
        "        estimator = cls(**params)\n",
        "        estimator._restore(os.path.join(path, 'model.ckpt'))\n",
        "        return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oiq35E893cmy",
        "colab_type": "text"
      },
      "source": [
        "# Train함수 생성 및 VAE 모델 훈련\n",
        "###2개의 Latent Space를 가진 VAE모델을 훈련시킵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X0acmBJkJYZ",
        "colab_type": "code",
        "outputId": "e7f85730-c9b0-4b40-fbc6-9ee0398d7dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "# Train 함수 정의\n",
        "def train(learning_rate=0.001, batch_size=100, training_epochs=10, display_step=5):\n",
        "    vae = VariationalAutoencoder(learning_rate=learning_rate,batch_size=batch_size)\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(n_samples / batch_size)\n",
        "        # 모든 배치들에 대하여 최적화 수행\n",
        "        for i in range(total_batch):\n",
        "            batch_xs, _ = mnist.train.next_batch(batch_size) # 배치사이즈에 맞게 x, y값 생성\n",
        "            \n",
        "            # Fit(opt, cost) training using batch data\n",
        "            cost = vae.fit(batch_xs)\n",
        "            # Compute average loss\n",
        "            avg_cost += cost / n_samples * batch_size\n",
        "\n",
        "        # Epoch당 로그 표시\n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \n",
        "                  \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "    return vae\n",
        "  \n",
        "vae2d = train(training_epochs=100)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e836b70a4507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mvae2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-e836b70a4507>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(learning_rate, batch_size, training_epochs, display_step)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Fit(opt, cost) training using batch data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;31m# Compute average loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-098526e7f565>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# 미니배치 모델을 훈련시킴(cost 반환)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mELBO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfASAsWr30Aj",
        "colab_type": "text"
      },
      "source": [
        "## Reconstruction 결과확인\n",
        "### x와 x'를 출력하여 얼마나 잘 복원됐는지 확인합니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76BUaeKHkNV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 입력MNIST 이미지와 복원된 MNIST 이미지를 8x12 figsize로 보여주는 plot 함수 정의\n",
        "x_sample = mnist.test.next_batch(100)[0]\n",
        "x_reconstruct = vae2d.reconstruct(x_sample)\n",
        "\n",
        "plt.figure(figsize=(8, 12))\n",
        "for i in range(5):\n",
        "    plt.subplot(5, 2, 2*i + 1)\n",
        "    plt.imshow(x_sample[i].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
        "    plt.title(\"Test input\")\n",
        "    plt.colorbar()\n",
        "    plt.subplot(5, 2, 2*i + 2)\n",
        "    plt.imshow(x_reconstruct[i].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
        "    plt.title(\"Reconstruction\")\n",
        "    plt.colorbar()\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQmvQj2t4tT1",
        "colab_type": "text"
      },
      "source": [
        "## Visualization(Latent space)\n",
        "### 훈련된 VAE의 2D Latent Space 위치해 있는 MNIST의 Manifold를 출력합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1OLkHlEkZPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 훈련된 모델의 MNIST Manifold를 20x20 Grid 로 보여주는 plot 정의\n",
        "nx = ny =20\n",
        "x_values = np.linspace(-3, 3, nx)\n",
        "y_values = np.linspace(-3, 3, ny)\n",
        "\n",
        "canvas = np.empty((28*ny, 28*nx))\n",
        "for i, yi in enumerate(x_values):\n",
        "    for j, xi in enumerate(y_values):\n",
        "        z_mu = np.array([[xi, yi]]*vae2d.batch_size)\n",
        "        x_mean = vae2d.generate(z_mu)\n",
        "        canvas[(nx-i-1)*28:(nx-i)*28, j*28:(j+1)*28] = x_mean[0].reshape(28, 28)\n",
        "\n",
        "plt.figure(figsize=(10, 10))        \n",
        "Xi, Yi = np.meshgrid(x_values, y_values)\n",
        "plt.title(\"Manifold\")\n",
        "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2a65Xbd5kKX",
        "colab_type": "text"
      },
      "source": [
        "### MNIST의 Latent Space에서 데이터 a,b의 Interpolation을 수행합니다. (b -> a)\n",
        "\n",
        "*   알파값을 잘 조정해야 합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS_LbZiVkUoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Interpolation between two classes a and b.   \n",
        "#  Z: np.array of the latent space with shape: (n_samples, latent_dim)\n",
        "#  labels: array of class labels (n_samples, ) \n",
        "def interpolate_from_a_to_b(Z, labels, generator, a, b, alpha = alphas, figsize=(12,5)):\n",
        "    \n",
        "    # Find the centroids of the classes a, b\n",
        "    z_a_avg = Z[labels == a].mean(axis=0)\n",
        "    z_b_avg = Z[labels == b].mean(axis=0)\n",
        "    \n",
        "    # Pick the medoid for class a for interpolation\n",
        "    z_a_med = np.median(Z[labels == a], axis=0)\n",
        "    \n",
        "    # The interpolation vector pointing from b -> a\n",
        "    z_b2a =  z_b_avg - z_a_avg \n",
        "\n",
        "    x_gens = []\n",
        "    \n",
        "    for alpha in alphas:\n",
        "        z_interp = z_a_med + alpha * z_b2a\n",
        "        x_gens.append(generator.generate(z_interp.reshape(1, -1)))\n",
        "\n",
        "    n = len(x_gens)\n",
        "    \n",
        "    #MNIST 이미지 figsize=(12,5) 출력\n",
        "    canvas = np.empty((28, 28*n))\n",
        "    for i, x in enumerate(x_gens):\n",
        "        # scale to (0, 1)\n",
        "        x = preprocessing.minmax_scale(x.T).T\n",
        "        canvas[:, i*28: (i+1)*28] = x[0].reshape(28, 28)\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.imshow(canvas, cmap=\"gray\")\n",
        "    ax.set_axis_off()\n",
        "    \n",
        "    return ax, x_gens\n",
        "  \n",
        "#Interpolation 실행  \n",
        "Z_test_vae2d = vae2d.transform(X_test)\n",
        "alphas = np.linspace(-10,10,20)\n",
        "#b->a \n",
        "ax, x_gens = interpolate_from_a_to_b(Z_test_vae2d, mnist.test.labels, vae2d,  \n",
        "                                    6,5, alphas)\n",
        "ax.set_title('VAE 20d')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FmztqZElx0Z",
        "colab_type": "text"
      },
      "source": [
        "## Reference\n",
        "-------\n",
        "*   https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\n",
        "*   https://nbviewer.jupyter.org/github/wangz10/Generative-Models/blob/master/Main.ipynb?source=post_page\n",
        "\n"
      ]
    }
  ]
}